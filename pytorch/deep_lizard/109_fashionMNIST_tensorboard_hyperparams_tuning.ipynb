{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fashion MNIST  \n",
    "---  \n",
    "## Tensorboard  \n",
    "https://www.youtube.com/watch?v=ycxulUVoNbk&list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG&index=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# from torchvision import transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120) # display option for output\n",
    "torch.set_grad_enabled(True) # gradient tracking turned on (default)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # pip install future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        # linear layer == fully connected layer == fc == dense layer\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer:\n",
    "        # t = t\n",
    "        \n",
    "        # (2) hidden conv layer:\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer:\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (4) hidden linear layer:\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        # t = t.flatten()\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (5) hidden linear layer:\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer:\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "## Training loop  \n",
    "Tweaking the hyperparamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 100 True\n",
      "0.1 100 False\n",
      "0.1 1000 True\n",
      "0.1 1000 False\n",
      "0.1 10000 True\n",
      "0.1 10000 False\n",
      "0.01 100 True\n",
      "0.01 100 False\n",
      "0.01 1000 True\n",
      "0.01 1000 False\n",
      "0.01 10000 True\n",
      "0.01 10000 False\n",
      "0.001 100 True\n",
      "0.001 100 False\n",
      "0.001 1000 True\n",
      "0.001 1000 False\n",
      "0.001 10000 True\n",
      "0.001 10000 False\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "batch_sizes = [100, 1000, 10000]\n",
    "lrs = [0.1, 0.01, 0.001]\n",
    "shuffles = [True, False]\n",
    "\n",
    "for lr, batch_size, shuffle in product(lrs, batch_sizes, shuffles):\n",
    "    print (lr, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_size=100 lr=0.1 shuffle=True\n",
      "epoch: 0 total correct: 5997 loss: 146071.41 accuracy 9.99%\n",
      "epoch: 1 total correct: 6027 loss: 138583.77 accuracy 10.04%\n",
      "epoch: 2 total correct: 6143 loss: 138567.50 accuracy 10.24%\n",
      "epoch: 3 total correct: 6069 loss: 138554.44 accuracy 10.12%\n",
      "epoch: 4 total correct: 6065 loss: 138632.04 accuracy 10.11%\n",
      " batch_size=100 lr=0.1 shuffle=False\n",
      "epoch: 0 total correct: 5985 loss: 156482.29 accuracy 9.97%\n",
      "epoch: 1 total correct: 5985 loss: 138615.24 accuracy 9.97%\n",
      "epoch: 2 total correct: 5995 loss: 138621.82 accuracy 9.99%\n",
      "epoch: 3 total correct: 5987 loss: 138624.36 accuracy 9.98%\n",
      "epoch: 4 total correct: 5982 loss: 138625.52 accuracy 9.97%\n",
      " batch_size=1000 lr=0.1 shuffle=True\n",
      "epoch: 0 total correct: 6057 loss: 253312.84 accuracy 10.10%\n",
      "epoch: 1 total correct: 6088 loss: 138300.55 accuracy 10.15%\n",
      "epoch: 2 total correct: 5902 loss: 138305.82 accuracy 9.84%\n",
      "epoch: 3 total correct: 5916 loss: 138272.17 accuracy 9.86%\n",
      "epoch: 4 total correct: 5838 loss: 138268.01 accuracy 9.73%\n",
      " batch_size=1000 lr=0.1 shuffle=False\n",
      "epoch: 0 total correct: 6026 loss: 176679.79 accuracy 10.04%\n",
      "epoch: 1 total correct: 6019 loss: 138316.76 accuracy 10.03%\n",
      "epoch: 2 total correct: 5968 loss: 138291.05 accuracy 9.95%\n",
      "epoch: 3 total correct: 6016 loss: 138280.53 accuracy 10.03%\n",
      "epoch: 4 total correct: 5953 loss: 138277.91 accuracy 9.92%\n",
      " batch_size=10000 lr=0.1 shuffle=True\n",
      "epoch: 0 total correct: 5855 loss: 329240.18 accuracy 9.76%\n",
      "epoch: 1 total correct: 5927 loss: 138742.63 accuracy 9.88%\n",
      "epoch: 2 total correct: 5983 loss: 138286.65 accuracy 9.97%\n",
      "epoch: 3 total correct: 6034 loss: 138233.44 accuracy 10.06%\n",
      "epoch: 4 total correct: 5973 loss: 138206.47 accuracy 9.96%\n",
      " batch_size=10000 lr=0.1 shuffle=False\n",
      "epoch: 0 total correct: 6099 loss: 417246.74 accuracy 10.16%\n",
      "epoch: 1 total correct: 5943 loss: 138790.18 accuracy 9.90%\n",
      "epoch: 2 total correct: 5956 loss: 138225.00 accuracy 9.93%\n",
      "epoch: 3 total correct: 6046 loss: 138232.65 accuracy 10.08%\n",
      "epoch: 4 total correct: 6071 loss: 138179.79 accuracy 10.12%\n",
      " batch_size=100 lr=0.01 shuffle=True\n",
      "epoch: 0 total correct: 46048 loss: 36302.08 accuracy 76.75%\n",
      "epoch: 1 total correct: 51156 loss: 23868.75 accuracy 85.26%\n",
      "epoch: 2 total correct: 51939 loss: 21524.65 accuracy 86.56%\n",
      "epoch: 3 total correct: 52147 loss: 20941.24 accuracy 86.91%\n",
      "epoch: 4 total correct: 52482 loss: 20246.62 accuracy 87.47%\n",
      " batch_size=100 lr=0.01 shuffle=False\n",
      "epoch: 0 total correct: 47579 loss: 32949.71 accuracy 79.30%\n",
      "epoch: 1 total correct: 51634 loss: 22767.49 accuracy 86.06%\n",
      "epoch: 2 total correct: 52354 loss: 20784.77 accuracy 87.26%\n",
      "epoch: 3 total correct: 52611 loss: 20090.85 accuracy 87.69%\n",
      "epoch: 4 total correct: 52841 loss: 19467.41 accuracy 88.07%\n",
      " batch_size=1000 lr=0.01 shuffle=True\n",
      "epoch: 0 total correct: 37754 loss: 57458.56 accuracy 62.92%\n",
      "epoch: 1 total correct: 47909 loss: 31944.32 accuracy 79.85%\n",
      "epoch: 2 total correct: 50050 loss: 26884.72 accuracy 83.42%\n",
      "epoch: 3 total correct: 51238 loss: 23734.79 accuracy 85.40%\n",
      "epoch: 4 total correct: 51911 loss: 21700.60 accuracy 86.52%\n",
      " batch_size=1000 lr=0.01 shuffle=False\n",
      "epoch: 0 total correct: 37843 loss: 57762.22 accuracy 63.07%\n",
      "epoch: 1 total correct: 47637 loss: 32158.68 accuracy 79.39%\n",
      "epoch: 2 total correct: 50006 loss: 26894.26 accuracy 83.34%\n",
      "epoch: 3 total correct: 51173 loss: 23946.15 accuracy 85.29%\n",
      "epoch: 4 total correct: 51866 loss: 21904.28 accuracy 86.44%\n",
      " batch_size=10000 lr=0.01 shuffle=True\n",
      "epoch: 0 total correct: 15107 loss: 126039.56 accuracy 25.18%\n",
      "epoch: 1 total correct: 23919 loss: 93135.47 accuracy 39.87%\n",
      "epoch: 2 total correct: 32978 loss: 68120.15 accuracy 54.96%\n",
      "epoch: 3 total correct: 38593 loss: 55440.58 accuracy 64.32%\n",
      "epoch: 4 total correct: 41423 loss: 47082.71 accuracy 69.04%\n",
      " batch_size=10000 lr=0.01 shuffle=False\n",
      "epoch: 0 total correct: 12959 loss: 130011.99 accuracy 21.60%\n",
      "epoch: 1 total correct: 28113 loss: 84080.11 accuracy 46.85%\n",
      "epoch: 2 total correct: 34549 loss: 63476.55 accuracy 57.58%\n",
      "epoch: 3 total correct: 39291 loss: 53302.65 accuracy 65.48%\n",
      "epoch: 4 total correct: 41429 loss: 47229.38 accuracy 69.05%\n",
      " batch_size=100 lr=0.001 shuffle=True\n",
      "epoch: 0 total correct: 41095 loss: 49169.21 accuracy 68.49%\n",
      "epoch: 1 total correct: 48170 loss: 31290.96 accuracy 80.28%\n",
      "epoch: 2 total correct: 50516 loss: 26391.64 accuracy 84.19%\n",
      "epoch: 3 total correct: 51452 loss: 23676.92 accuracy 85.75%\n",
      "epoch: 4 total correct: 52163 loss: 21693.59 accuracy 86.94%\n",
      " batch_size=100 lr=0.001 shuffle=False\n",
      "epoch: 0 total correct: 42554 loss: 45909.03 accuracy 70.92%\n",
      "epoch: 1 total correct: 48618 loss: 30230.22 accuracy 81.03%\n",
      "epoch: 2 total correct: 50457 loss: 26093.15 accuracy 84.09%\n",
      "epoch: 3 total correct: 51350 loss: 23731.33 accuracy 85.58%\n",
      "epoch: 4 total correct: 51879 loss: 22098.54 accuracy 86.47%\n",
      " batch_size=1000 lr=0.001 shuffle=True\n",
      "epoch: 0 total correct: 28726 loss: 95690.95 accuracy 47.88%\n",
      "epoch: 1 total correct: 42012 loss: 47745.56 accuracy 70.02%\n",
      "epoch: 2 total correct: 44785 loss: 40281.54 accuracy 74.64%\n",
      "epoch: 3 total correct: 46025 loss: 37022.81 accuracy 76.71%\n",
      "epoch: 4 total correct: 46882 loss: 34602.55 accuracy 78.14%\n",
      " batch_size=1000 lr=0.001 shuffle=False\n",
      "epoch: 0 total correct: 28823 loss: 94949.08 accuracy 48.04%\n",
      "epoch: 1 total correct: 42608 loss: 45978.82 accuracy 71.01%\n",
      "epoch: 2 total correct: 44839 loss: 39408.48 accuracy 74.73%\n",
      "epoch: 3 total correct: 46189 loss: 36148.33 accuracy 76.98%\n",
      "epoch: 4 total correct: 47155 loss: 33872.83 accuracy 78.59%\n",
      " batch_size=10000 lr=0.001 shuffle=True\n",
      "epoch: 0 total correct: 9945 loss: 137787.52 accuracy 16.57%\n",
      "epoch: 1 total correct: 17827 loss: 135578.09 accuracy 29.71%\n",
      "epoch: 2 total correct: 27505 loss: 128654.69 accuracy 45.84%\n",
      "epoch: 3 total correct: 31128 loss: 111649.04 accuracy 51.88%\n",
      "epoch: 4 total correct: 33695 loss: 86059.17 accuracy 56.16%\n",
      " batch_size=10000 lr=0.001 shuffle=False\n",
      "epoch: 0 total correct: 6211 loss: 137872.20 accuracy 10.35%\n",
      "epoch: 1 total correct: 18182 loss: 135748.73 accuracy 30.30%\n",
      "epoch: 2 total correct: 23005 loss: 129190.01 accuracy 38.34%\n",
      "epoch: 3 total correct: 25831 loss: 113495.93 accuracy 43.05%\n",
      "epoch: 4 total correct: 29567 loss: 89063.84 accuracy 49.28%\n"
     ]
    }
   ],
   "source": [
    "for lr, batch_size, shuffle in product(lrs, batch_sizes, shuffles):\n",
    "    \n",
    "    network = Network()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    images, labels = next(iter(train_loader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    comment = f' batch_size={batch_size} lr={lr} shuffle={shuffle}'\n",
    "    tb = SummaryWriter(comment=comment)\n",
    "    tb.add_image('images', grid)\n",
    "    tb.add_graph(network, images)\n",
    "    print(comment)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels) # calculating the loss function\n",
    "\n",
    "            optimizer.zero_grad() # zero out the gradients, because pytorch is actually adding the grads\n",
    "            loss.backward() # calculating the gradients\n",
    "            optimizer.step() # update the weight\n",
    "\n",
    "            total_loss += loss.item() * batch_size # total loss is dependent on batch_size\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        tb.add_scalar('Loss', total_loss, epoch)\n",
    "        tb.add_scalar('Correct', total_correct, epoch)\n",
    "        tb.add_scalar('Accuracy', 100*total_correct/len(train_set), epoch)\n",
    "\n",
    "        # tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "        # tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "        # tb.add_histogram('conv1.weight.grad', network.conv1.weight.grad, epoch)\n",
    "\n",
    "        for name, weight in network.named_parameters():\n",
    "            tb.add_histogram(name, weight, epoch)\n",
    "            tb.add_histogram(f'{name}.grad', weight.grad, epoch)\n",
    "\n",
    "        print(f'epoch: {epoch} total correct: {total_correct} loss: {total_loss:.2f} accuracy {100*total_correct / len(train_set):.2f}%')\n",
    "\n",
    "    tb.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
